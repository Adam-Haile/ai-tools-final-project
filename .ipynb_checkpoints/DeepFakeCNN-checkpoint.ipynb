{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-18 20:08:00.777185: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-18 20:08:01.408470: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow\n",
    "from tqdm import tqdm\n",
    "import tensorflow.keras as keras\n",
    "import frame_extractor as extractor\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 401/401 [01:06<00:00,  6.04it/s]\n"
     ]
    }
   ],
   "source": [
    "train_X = []\n",
    "train_Y = []\n",
    "parent = os.path.dirname(os.getcwd())\n",
    "videos = os.listdir(parent + \"/train_sample_videos/\")\n",
    "f = open(parent + \"/train_sample_videos/metadata.json\")\n",
    "valid = json.load(f)\n",
    "for video in tqdm(videos):\n",
    "    if video != \"metadata.json\":\n",
    "        for frame in extractor.run_extraction(parent + \"/train_sample_videos/\" + video):\n",
    "            train_X.append(frame)\n",
    "            break\n",
    "        train_Y.append(1 if valid[video][\"label\"] == \"FAKE\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_orientation(train_X):\n",
    "    h_sum = 0\n",
    "    v_sum = 0\n",
    "    uhoh_sum = 0\n",
    "    for nar in train_X:\n",
    "        if(nar.shape==(1920,1080,3)):\n",
    "            h_sum+=1\n",
    "        elif(nar.shape==(1080,1920,3)):\n",
    "            v_sum+=1\n",
    "        else:\n",
    "            uhoh_sum+=1\n",
    "\n",
    "\n",
    "    print(h_sum)\n",
    "    print(v_sum)\n",
    "    print(uhoh_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_vertical(x):\n",
    "    if x.shape == (1920,1080,3):\n",
    "        return np.rot90(x)\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotated_train_X = list(map(rotate_vertical, train_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = rotated_train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ignore this for now, I don't think we want to filter verticals, \n",
    "#but maybe in the future we'll have to, so this code is nice as a failsafe\n",
    "# def filter_horizontal(x):\n",
    "#     return x.shape==(1080,1920,3)\n",
    "# only_horizontal = list(filter(filter_horizontal, train_X))\n",
    "# check_orientation(only_horizontal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Model\n",
    "This cell creates the cnn model to be trained. (Currently a basic CNN example model, not our finished model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_epochs = 10\n",
    "cnn_batch_size = 1\n",
    "train_samples = len(train_X)#TODO: Input number of samples: something like \"len(x_train)\", if x_train is our training array\n",
    "input_shape = (1080,1920,3) #TODO: Set input_shape to the shape of our input\n",
    "num_classes = 2\n",
    "\n",
    "steps_per_epoch = train_samples/cnn_batch_size\n",
    "\n",
    "cnn_model = tensorflow.keras.models.Sequential()\n",
    "cnn_model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape)) \n",
    "cnn_model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn_model.add(Dropout(0.25))\n",
    "cnn_model.add(Flatten())\n",
    "# cnn_model.add(Dense(128, activation='relu'))\n",
    "cnn_model.add(Dense(num_classes, activation='softmax'))\n",
    "cnn_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-18 20:12:01.047141: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:1014] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_1/dropout_1/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "2023-11-18 20:12:11.253611: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 7.89GiB (rounded to 8468905984)requested by op sequential_1/conv2d_2/Relu\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-11-18 20:12:11.253704: I tensorflow/tsl/framework/bfc_allocator.cc:1039] BFCAllocator dump for GPU_0_bfc\n",
      "2023-11-18 20:12:11.253733: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (256): \tTotal Chunks: 50, Chunks in use: 50. 12.5KiB allocated for chunks. 12.5KiB in use in bin. 2.1KiB client-requested in use in bin.\n",
      "2023-11-18 20:12:11.253752: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-11-18 20:12:11.253769: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (1024): \tTotal Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\n",
      "2023-11-18 20:12:11.253787: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (2048): \tTotal Chunks: 5, Chunks in use: 5. 17.2KiB allocated for chunks. 17.2KiB in use in bin. 16.6KiB client-requested in use in bin.\n",
      "2023-11-18 20:12:11.253803: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (4096): \tTotal Chunks: 1, Chunks in use: 1. 4.8KiB allocated for chunks. 4.8KiB in use in bin. 3.1KiB client-requested in use in bin.\n",
      "2023-11-18 20:12:11.253818: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-11-18 20:12:11.253832: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-11-18 20:12:11.253846: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-11-18 20:12:11.253864: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (65536): \tTotal Chunks: 2, Chunks in use: 2. 144.0KiB allocated for chunks. 144.0KiB in use in bin. 144.0KiB client-requested in use in bin.\n",
      "2023-11-18 20:12:11.253882: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (131072): \tTotal Chunks: 2, Chunks in use: 2. 280.0KiB allocated for chunks. 280.0KiB in use in bin. 144.0KiB client-requested in use in bin.\n",
      "2023-11-18 20:12:11.253897: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-11-18 20:12:11.253911: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-11-18 20:12:11.253925: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-11-18 20:12:11.253939: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-11-18 20:12:11.253953: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-11-18 20:12:11.253966: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-11-18 20:12:11.253980: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-11-18 20:12:11.253994: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-11-18 20:12:11.254008: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-11-18 20:12:11.254026: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (134217728): \tTotal Chunks: 6, Chunks in use: 4. 1.47GiB allocated for chunks. 1006.65MiB in use in bin. 1006.65MiB client-requested in use in bin.\n",
      "2023-11-18 20:12:11.254047: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (268435456): \tTotal Chunks: 4, Chunks in use: 3. 12.00GiB allocated for chunks. 5.38GiB in use in bin. 5.38GiB client-requested in use in bin.\n",
      "2023-11-18 20:12:11.254065: I tensorflow/tsl/framework/bfc_allocator.cc:1062] Bin for 7.89GiB was 256.00MiB, Chunk State: \n",
      "2023-11-18 20:12:11.254089: I tensorflow/tsl/framework/bfc_allocator.cc:1068]   Size: 6.62GiB | Requested Size: 0B | in_use: 0 | bin_num: 20, prev:   Size: 759.38MiB | Requested Size: 759.38MiB | in_use: 1 | bin_num: -1\n",
      "2023-11-18 20:12:11.254101: I tensorflow/tsl/framework/bfc_allocator.cc:1075] Next region of size 14467596288\n",
      "2023-11-18 20:12:11.254117: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3eb2000000 of size 256 next 1\n",
      "2023-11-18 20:12:11.254131: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3eb2000100 of size 1280 next 2\n",
      "2023-11-18 20:12:11.254143: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3eb2000600 of size 256 next 3\n",
      "2023-11-18 20:12:11.254155: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3eb2000700 of size 256 next 4\n",
      "2023-11-18 20:12:11.254167: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3eb2000800 of size 256 next 6\n",
      "2023-11-18 20:12:11.254179: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3eb2000900 of size 256 next 7\n",
      "2023-11-18 20:12:11.254191: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3eb2000a00 of size 256 next 5\n",
      "2023-11-18 20:12:11.254202: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3eb2000b00 of size 256 next 8\n",
      "2023-11-18 20:12:11.254214: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3eb2000c00 of size 256 next 13\n",
      "2023-11-18 20:12:11.254227: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3eb2000d00 of size 256 next 11\n",
      "2023-11-18 20:12:11.254239: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3eb2000e00 of size 256 next 12\n",
      "2023-11-18 20:12:11.254250: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3eb2000f00 of size 256 next 18\n",
      "2023-11-18 20:12:11.254262: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3eb2001000 of size 256 next 16\n",
      "2023-11-18 20:12:11.254275: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3eb2001100 of size 256 next 17\n",
      "2023-11-18 20:12:11.254287: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3eb2001200 of size 256 next 21\n",
      "2023-11-18 20:12:11.254298: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3eb2001300 of size 256 next 22\n",
      "2023-11-18 20:12:11.254311: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3eb2001400 of size 4864 next 9\n",
      "2023-11-18 20:12:11.254325: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3eb2002700 of size 3584 next 10\n",
      "2023-11-18 20:12:11.254337: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3eb2003500 of size 256 next 24\n",
      "2023-11-18 20:12:11.254349: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3eb2003600 of size 256 next 25\n",
      "2023-11-18 20:12:11.254361: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3eb2003700 of size 256 next 26\n",
      "2023-11-18 20:12:11.254374: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3eb2003800 of size 3584 next 27\n",
      "2023-11-18 20:12:11.254386: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3eb2004600 of size 256 next 28\n",
      "2023-11-18 20:12:11.254399: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3eb2004700 of size 142848 next 15\n",
      "2023-11-18 20:12:11.254411: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3eb2027500 of size 73728 next 14\n",
      "2023-11-18 20:12:11.254425: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3eb2039500 of size 256 next 29\n",
      "2023-11-18 20:12:11.254445: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3eb2039600 of size 263886848 next 30\n",
      "2023-11-18 20:12:11.254458: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3ec1be2e00 of size 256 next 31\n",
      "2023-11-18 20:12:11.254476: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3ec1be2f00 of size 256 next 32\n",
      "2023-11-18 20:12:11.254488: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3ec1be3000 of size 256 next 33\n",
      "2023-11-18 20:12:11.254508: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3ec1be3100 of size 256 next 34\n",
      "2023-11-18 20:12:11.254520: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3ec1be3200 of size 256 next 35\n",
      "2023-11-18 20:12:11.254532: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3ec1be3300 of size 256 next 36\n",
      "2023-11-18 20:12:11.254549: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3ec1be3400 of size 256 next 37\n",
      "2023-11-18 20:12:11.254561: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3ec1be3500 of size 256 next 38\n",
      "2023-11-18 20:12:11.254574: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3ec1be3600 of size 256 next 39\n",
      "2023-11-18 20:12:11.254590: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3ec1be3700 of size 256 next 40\n",
      "2023-11-18 20:12:11.254603: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3ec1be3800 of size 256 next 43\n",
      "2023-11-18 20:12:11.254621: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3ec1be3900 of size 256 next 44\n",
      "2023-11-18 20:12:11.254634: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3ec1be3a00 of size 256 next 49\n",
      "2023-11-18 20:12:11.254646: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3ec1be3b00 of size 256 next 41\n",
      "2023-11-18 20:12:11.254659: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3ec1be3c00 of size 256 next 42\n",
      "2023-11-18 20:12:11.254676: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3ec1be3d00 of size 3328 next 53\n",
      "2023-11-18 20:12:11.254688: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3ec1be4a00 of size 256 next 55\n",
      "2023-11-18 20:12:11.254707: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3ec1be4b00 of size 256 next 56\n",
      "2023-11-18 20:12:11.254728: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3ec1be4c00 of size 256 next 58\n",
      "2023-11-18 20:12:11.254740: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3ec1be4d00 of size 256 next 59\n",
      "2023-11-18 20:12:11.254757: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3ec1be4e00 of size 256 next 60\n",
      "2023-11-18 20:12:11.254769: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3ec1be4f00 of size 256 next 61\n",
      "2023-11-18 20:12:11.254781: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3ec1be5000 of size 256 next 62\n",
      "2023-11-18 20:12:11.254794: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3ec1be5100 of size 256 next 63\n",
      "2023-11-18 20:12:11.254813: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3ec1be5200 of size 256 next 64\n",
      "2023-11-18 20:12:11.254825: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3ec1be5300 of size 256 next 65\n",
      "2023-11-18 20:12:11.254842: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3ec1be5400 of size 256 next 66\n",
      "2023-11-18 20:12:11.254855: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3ec1be5500 of size 256 next 67\n",
      "2023-11-18 20:12:11.254872: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3ec1be5600 of size 256 next 47\n",
      "2023-11-18 20:12:11.254884: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3ec1be5700 of size 3584 next 45\n",
      "2023-11-18 20:12:11.254922: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3ec1be6500 of size 3584 next 54\n",
      "2023-11-18 20:12:11.254935: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3ec1be7300 of size 143872 next 48\n",
      "2023-11-18 20:12:11.254948: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3ec1c0a500 of size 73728 next 46\n",
      "2023-11-18 20:12:11.254967: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3ec1c1c500 of size 256 next 71\n",
      "2023-11-18 20:12:11.254979: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f3ec1c1c600 of size 263651072 next 20\n",
      "2023-11-18 20:12:11.254998: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3ed178c500 of size 263886848 next 19\n",
      "2023-11-18 20:12:11.255010: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3ee1335d00 of size 2488320000 next 23\n",
      "2023-11-18 20:12:11.255028: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3f75841d00 of size 263886848 next 57\n",
      "2023-11-18 20:12:11.255041: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3f853eb500 of size 256 next 68\n",
      "2023-11-18 20:12:11.255058: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f3f853eb600 of size 263886592 next 51\n",
      "2023-11-18 20:12:11.255070: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3f94f94d00 of size 263886848 next 50\n",
      "2023-11-18 20:12:11.255088: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f3fa4b3e500 of size 2488320000 next 52\n",
      "2023-11-18 20:12:11.255101: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f403904a500 of size 796262400 next 70\n",
      "2023-11-18 20:12:11.255119: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f40687aa500 of size 7111138048 next 18446744073709551615\n",
      "2023-11-18 20:12:11.255130: I tensorflow/tsl/framework/bfc_allocator.cc:1100]      Summary of in-use Chunks by size: \n",
      "2023-11-18 20:12:11.255153: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 50 Chunks of size 256 totalling 12.5KiB\n",
      "2023-11-18 20:12:11.255168: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2023-11-18 20:12:11.255189: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 3328 totalling 3.2KiB\n",
      "2023-11-18 20:12:11.255203: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 4 Chunks of size 3584 totalling 14.0KiB\n",
      "2023-11-18 20:12:11.255222: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 4864 totalling 4.8KiB\n",
      "2023-11-18 20:12:11.255243: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 73728 totalling 144.0KiB\n",
      "2023-11-18 20:12:11.255265: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 142848 totalling 139.5KiB\n",
      "2023-11-18 20:12:11.255287: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 143872 totalling 140.5KiB\n",
      "2023-11-18 20:12:11.255309: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 4 Chunks of size 263886848 totalling 1006.65MiB\n",
      "2023-11-18 20:12:11.255331: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 796262400 totalling 759.38MiB\n",
      "2023-11-18 20:12:11.255352: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 2488320000 totalling 4.63GiB\n",
      "2023-11-18 20:12:11.255365: I tensorflow/tsl/framework/bfc_allocator.cc:1107] Sum Total of in-use chunks: 6.36GiB\n",
      "2023-11-18 20:12:11.255384: I tensorflow/tsl/framework/bfc_allocator.cc:1109] Total bytes in pool: 14467596288 memory_limit_: 14467596288 available bytes: 0 curr_region_allocation_bytes_: 28935192576\n",
      "2023-11-18 20:12:11.255414: I tensorflow/tsl/framework/bfc_allocator.cc:1114] Stats: \n",
      "Limit:                     14467596288\n",
      "InUse:                      6828920576\n",
      "MaxInUse:                  13077766400\n",
      "NumAllocs:                         109\n",
      "MaxAllocSize:               8468905984\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2023-11-18 20:12:11.255445: W tensorflow/tsl/framework/bfc_allocator.cc:497] **_**********************_*************************_________________________________________________\n",
      "2023-11-18 20:12:11.255484: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at conv_ops_fused_impl.h:770 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[32,1078,1918,32] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-11-18 20:12:11.255527: I tensorflow/core/common_runtime/executor.cc:1209] [/job:localhost/replica:0/task:0/device:GPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[32,1078,1918,32] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node sequential_1/conv2d_2/Relu}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n",
      "\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'sequential_1/conv2d_2/Relu' defined at (most recent call last):\n    File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 725, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 600, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1896, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n      await self.process_one()\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 502, in process_one\n      await dispatch(*args)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n      await result\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_1705757/540485387.py\", line 1, in <module>\n      cnn_model.fit(x=np.array(train_X),y=np.array(train_Y),epochs=cnn_epochs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1050, in train_step\n      y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 558, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/sequential.py\", line 412, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/functional.py\", line 512, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/functional.py\", line 669, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/layers/convolutional/base_conv.py\", line 321, in call\n      return self.activation(outputs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/activations.py\", line 317, in relu\n      return backend.relu(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/backend.py\", line 5396, in relu\n      x = tf.nn.relu(x)\nNode: 'sequential_1/conv2d_2/Relu'\nOOM when allocating tensor with shape[32,1078,1918,32] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node sequential_1/conv2d_2/Relu}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_1517]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcnn_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_X\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_Y\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcnn_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'sequential_1/conv2d_2/Relu' defined at (most recent call last):\n    File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 725, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 600, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1896, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n      await self.process_one()\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 502, in process_one\n      await dispatch(*args)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n      await result\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_1705757/540485387.py\", line 1, in <module>\n      cnn_model.fit(x=np.array(train_X),y=np.array(train_Y),epochs=cnn_epochs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1050, in train_step\n      y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 558, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/sequential.py\", line 412, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/functional.py\", line 512, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/functional.py\", line 669, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/layers/convolutional/base_conv.py\", line 321, in call\n      return self.activation(outputs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/activations.py\", line 317, in relu\n      return backend.relu(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/backend.py\", line 5396, in relu\n      x = tf.nn.relu(x)\nNode: 'sequential_1/conv2d_2/Relu'\nOOM when allocating tensor with shape[32,1078,1918,32] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node sequential_1/conv2d_2/Relu}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_1517]"
     ]
    }
   ],
   "source": [
    "cnn_model.fit(x=np.array(train_X),y=np.array(train_Y),epochs=cnn_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
