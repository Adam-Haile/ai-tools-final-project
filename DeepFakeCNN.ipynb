{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project \n",
    "\n",
    "**Title**: Deep Fake Detection\n",
    "\n",
    "**Contributors**: Adam Haile, Alhagie Boye, Rudolph Evonich\n",
    "\n",
    "**Onjective**: To develop a robust CNN model capable of accurately classifying video frames as real or fake.\n",
    "\n",
    "\n",
    "This is a demo of our CNN model that will be use to classify videos as real or fake.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "import tensorflow.keras as keras\n",
    "import frame_extractor as extractor\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading & Data Preprocessing\n",
    "The following cells handles the loading of video data and associated metadata and data preprocessing.\n",
    "\n",
    "Video files and their corresponding metadata are retrieved from a designated directory (train_sample_videos/) and a JSON file (metadata.json).\n",
    "\n",
    "A custom module named frame_extractor is utilized to extract frame from each video for further processing.\n",
    "\n",
    "The orientation of the frames is checked, and those with vertical orientation are rotated to ensure consistency.\n",
    "\n",
    "Based on the metadata information, binary labels are assigned to each frame: 1 for \"FAKE\" videos and 0 for \"REAL\" videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_vertical(x):\n",
    "    if x.shape == (1920,1080,3):\n",
    "        return np.rot90(x)\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = []\n",
    "train_Y = []\n",
    "parent = os.path.dirname(os.getcwd())\n",
    "videos = glob.glob(parent + \"/train_sample_videos/*.mp4\")\n",
    "f = open(parent + \"/train_sample_videos/metadata.json\")\n",
    "valid = json.load(f)\n",
    "\n",
    "videos_processed = 0\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=len(videos)) as executor:\n",
    "    futures = {executor.submit(extractor.run_extraction, directory): directory for directory in videos}\n",
    "    concurrent.futures.wait(futures)\n",
    "    \n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        directory = futures[future]\n",
    "        video_name = directory.split(\"/\")[-1]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            for frame in result:\n",
    "                train_X.append(rotate_vertical(frame))\n",
    "                train_Y.append(np.array([1,0]) if valid[video_name][\"label\"] == \"FAKE\" else np.array([0,1]))\n",
    "            videos_processed += 1\n",
    "            print (f\"Videos Finished: {videos_processed} / {len(videos)}\", end='\\r')\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing directory {directory}: {e}\")\n",
    "            \n",
    "train_X = np.array(train_X)\n",
    "train_Y = np.array(train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation and Training\n",
    "This cell creates the cnn model to be trained. (Currently a basic CNN example model, not our finished model)\n",
    "\n",
    "This serves as a basic example and comprises a convolutional layer, max-pooling, flattening, and two dense layers.\n",
    "\n",
    "The dataset is split into training and validation sets to assess the model's performance during training.\n",
    "\n",
    "The model is trained on the training set for a specified number of epochs and batch size.\n",
    "\n",
    "The training and validation accuracy are visualized over epochs using matplotlib to track the model's learning progress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn_epochs = 10\n",
    "cnn_batch_size = 5\n",
    "train_samples = len(train_X)\n",
    "input_shape =  (1080,1920,3) #TODO: Set input_shape to the shape of our input\n",
    "num_classes = 2\n",
    "\n",
    "steps_per_epoch = train_samples/cnn_batch_size\n",
    "\n",
    "# Split the data \n",
    "train_X, validation_X, train_Y, validation_Y = train_test_split(train_X, train_Y, test_size=0.2, random_state=42)\n",
    "\n",
    "cnn_model = tensorflow.keras.models.Sequential()\n",
    "cnn_model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(Dense(16, activation='relu'))  # Reduce the number of neurons in this layer\n",
    "cnn_model.add(Dense(num_classes, activation='softmax'))\n",
    "cnn_model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(),\n",
    "              metrics=['accuracy'])                     \n",
    "cnn_model.summary()\n",
    "\n",
    "history = cnn_model.fit(train_X, train_Y, epochs=cnn_epochs, batch_size=cnn_batch_size, validation_data=(validation_X, validation_Y))\n",
    "\n",
    "# Plot for the experiment\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# The code elow evaluates the trained CNN model's performance on the validation set.\n",
    "# The model's ability to distinguish between real and fake video frames is assessed using the validation data.\n",
    "# The validation accuracy is calculated and displayed, providing an indication of the model's generalizability\n",
    "evaluation = cnn_model.evaluate(validation_X, validation_Y)\n",
    "print(f\"Validation Accuracy: {evaluation[1] * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
